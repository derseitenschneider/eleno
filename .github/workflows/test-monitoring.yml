name: 📊 Test Monitoring & Analytics

on:
  # Run weekly reports on Monday mornings
  schedule:
    - cron: '0 8 * * 1'  # Weekly on Monday 8 AM UTC
    - cron: '0 8 1 * *'  # Monthly on 1st day 8 AM UTC
  
  # Allow manual execution with customizable options
  workflow_dispatch:
    inputs:
      report_type:
        description: 'Report period'
        required: true
        default: 'week'
        type: choice
        options:
          - week
          - month
      include_performance:
        description: 'Include performance benchmark'
        required: false
        default: true
        type: boolean
      send_notifications:
        description: 'Send notifications'
        required: false
        default: true
        type: boolean

  # Trigger on main branch pushes for immediate analysis
  push:
    branches: [main]
    paths:
      - "app/**"
      - ".github/workflows/test-monitoring.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  NODE_VERSION: "22"
  CACHE_VERSION: "v1"

jobs:
  # Job 1: Collect and Analyze Current Metrics
  collect-metrics:
    name: 📊 Collect Test Metrics
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ./app
    
    steps:
      - name: 🚚 Checkout code
        uses: actions/checkout@v4
        with:
          # Get full history for trend analysis
          fetch-depth: 50

      - name: 📦 Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: app/package-lock.json

      - name: 💾 Cache node modules
        uses: actions/cache@v4
        with:
          path: app/node_modules
          key: ${{ runner.os }}-node-${{ env.CACHE_VERSION }}-${{ hashFiles('app/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-node-

      - name: 📥 Install dependencies
        run: npm ci

      - name: 📈 Restore existing metrics
        uses: actions/cache/restore@v4
        with:
          path: app/.test-metrics
          key: test-metrics-${{ github.repository }}-${{ github.ref_name }}
          restore-keys: |
            test-metrics-${{ github.repository }}-main
            test-metrics-${{ github.repository }}-

      - name: 🧪 Run tests with coverage
        run: npm run test:cov
        continue-on-error: true

      - name: 🎭 Run sample E2E tests for metrics
        run: npm run pw -- --project="*performance*" --reporter=list
        continue-on-error: true
        timeout-minutes: 15
        env:
          VITE_ENV: staging

      - name: 📊 Collect current metrics
        run: |
          node scripts/test-metrics-collector.js collect
          node scripts/test-metrics-collector.js trends
          node scripts/test-metrics-collector.js alerts

      - name: 🚨 Detect and analyze alerts
        id: alerts
        run: |
          node scripts/test-alerting-system.js check || echo "alerts_detected=true" >> $GITHUB_OUTPUT
          node scripts/test-alerting-system.js summary >> $GITHUB_STEP_SUMMARY

      - name: 💾 Save metrics cache
        uses: actions/cache/save@v4
        if: always()
        with:
          path: app/.test-metrics
          key: test-metrics-${{ github.repository }}-${{ github.ref_name }}-${{ github.run_id }}

      - name: 📤 Upload metrics data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-metrics-${{ github.run_id }}
          path: |
            app/.test-metrics/
            app/coverage/
            app/test-results/
          retention-days: 30

  # Job 2: Performance Benchmarking (optional)
  performance-benchmark:
    name: ⚡ Performance Benchmark
    runs-on: ubuntu-latest
    needs: collect-metrics
    if: inputs.include_performance != false
    defaults:
      run:
        working-directory: ./app
        
    steps:
      - name: 🚚 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: app/package-lock.json

      - name: 💾 Restore node modules cache
        uses: actions/cache@v4
        with:
          path: app/node_modules
          key: ${{ runner.os }}-node-${{ env.CACHE_VERSION }}-${{ hashFiles('app/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-node-

      - name: 📥 Install dependencies
        run: npm ci

      - name: 📈 Restore metrics data
        uses: actions/download-artifact@v4
        with:
          name: test-metrics-${{ github.run_id }}
          path: ./app/

      - name: ⚡ Run performance benchmark
        run: |
          node scripts/performance-monitor.js benchmark
          node scripts/performance-monitor.js regressions || echo "regressions_detected=true" >> $GITHUB_OUTPUT

      - name: 💡 Generate optimization recommendations
        run: |
          echo "## ⚡ Performance Recommendations" >> $GITHUB_STEP_SUMMARY
          node scripts/performance-monitor.js recommendations >> $GITHUB_STEP_SUMMARY

      - name: 📤 Upload performance data
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-data-${{ github.run_id }}
          path: app/.test-metrics/performance/
          retention-days: 30

  # Job 3: Generate Dashboard and Reports
  generate-reports:
    name: 📈 Generate Reports & Dashboard
    runs-on: ubuntu-latest
    needs: [collect-metrics]
    if: always()
    defaults:
      run:
        working-directory: ./app
        
    steps:
      - name: 🚚 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: 📈 Download metrics data
        uses: actions/download-artifact@v4
        with:
          name: test-metrics-${{ github.run_id }}
          path: ./app/

      - name: 📈 Download performance data (if available)
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: performance-data-${{ github.run_id }}
          path: ./app/.test-metrics/performance/

      - name: 🎨 Generate visual dashboard
        run: |
          node scripts/test-dashboard-generator.js generate
          echo "Dashboard generated at: .test-metrics/dashboard/index.html"

      - name: 📊 Generate automated reports
        run: |
          # Determine report type based on schedule or input
          REPORT_TYPE="${{ inputs.report_type }}"
          if [ -z "$REPORT_TYPE" ]; then
            # If it's the 1st of the month, generate monthly report
            if [ "$(date +%d)" = "01" ]; then
              REPORT_TYPE="month"
            else
              REPORT_TYPE="week"
            fi
          fi
          
          echo "Generating $REPORT_TYPE report..."
          node scripts/test-reporting-automation.js generate $REPORT_TYPE

      - name: 📋 Generate monitoring summary
        run: |
          echo "# 📊 Test Monitoring Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Dashboard link (if we could host it)
          echo "## 📈 Dashboard" >> $GITHUB_STEP_SUMMARY
          echo "Interactive dashboard generated with current metrics and trends." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Quality score
          QUALITY_SCORE=$(node scripts/test-reporting-automation.js quality week | head -1)
          echo "## 🎯 $QUALITY_SCORE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Current metrics summary
          node scripts/test-dashboard-generator.js text >> $GITHUB_STEP_SUMMARY
          
          # Links to artifacts
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## 📦 Available Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- [Test Metrics Data](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY
          echo "- [Interactive Dashboard](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY
          echo "- [Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY
          echo "- [Automated Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY

      - name: 📤 Upload dashboard and reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: monitoring-dashboard-${{ github.run_id }}
          path: |
            app/.test-metrics/dashboard/
            app/.test-metrics/reports/
          retention-days: 90

  # Job 4: Send Notifications (if enabled)
  send-notifications:
    name: 📢 Send Notifications
    runs-on: ubuntu-latest
    needs: [collect-metrics, generate-reports]
    if: (inputs.send_notifications != false) && (needs.collect-metrics.outputs.alerts_detected == 'true' || github.event_name == 'schedule')
    defaults:
      run:
        working-directory: ./app
        
    environment:
      name: notifications
      
    steps:
      - name: 🚚 Checkout code
        uses: actions/checkout@v4

      - name: 📦 Setup Node.js ${{ env.NODE_VERSION }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: 📈 Download metrics data
        uses: actions/download-artifact@v4
        with:
          name: test-metrics-${{ github.run_id }}
          path: ./app/

      - name: 📢 Send alert notifications
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          ALERT_EMAIL_FROM: ${{ secrets.ALERT_EMAIL_FROM }}
          ALERT_EMAIL_TO: ${{ secrets.ALERT_EMAIL_TO }}
        run: |
          echo "📨 Sending notifications to configured channels..."
          node scripts/test-alerting-system.js notify || echo "Some notifications may have failed"

      - name: 💬 Create GitHub issue for high-priority alerts
        if: needs.collect-metrics.outputs.alerts_detected == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # Check for high-priority alerts and create GitHub issue if needed
          HIGH_ALERTS=$(node scripts/test-alerting-system.js check 2>&1 | grep -c "HIGH:" || echo "0")
          
          if [ "$HIGH_ALERTS" -gt 0 ]; then
            echo "Creating GitHub issue for $HIGH_ALERTS high-priority alerts..."
            
            ISSUE_BODY="## 🚨 High Priority Test Alerts Detected
            
            **Workflow Run:** [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            **Branch:** \`${{ github.ref_name }}\`
            **Commit:** [\`${{ github.sha }}\`](https://github.com/${{ github.repository }}/commit/${{ github.sha }})
            
            The following high-priority test alerts were detected:
            
            \`\`\`
            $(node scripts/test-alerting-system.js summary)
            \`\`\`
            
            ## 📊 Current Quality Score
            
            \`\`\`
            $(node scripts/test-reporting-automation.js quality week)
            \`\`\`
            
            ## 🔗 Resources
            
            - [Test Dashboard](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)
            - [Performance Reports](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)
            - [Monitoring Guide](./docs/TEST_MONITORING_GUIDE.md)
            
            ---
            *This issue was automatically created by the test monitoring system.*"
            
            gh issue create \
              --title "🚨 High Priority Test Alerts - $(date +'%Y-%m-%d')" \
              --body "$ISSUE_BODY" \
              --label "bug,monitoring,high-priority" \
              --assignee "${{ github.actor }}"
          fi

  # Job 5: Update Monitoring Status
  update-status:
    name: 📊 Update Monitoring Status
    runs-on: ubuntu-latest
    needs: [collect-metrics, generate-reports, send-notifications]
    if: always()
    
    steps:
      - name: 📊 Final monitoring summary
        run: |
          echo "# 📊 Test Monitoring Workflow Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run**: [${{ github.run_id }}](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Branch**: ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Timestamp**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## Job Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Collect Metrics | ${{ needs.collect-metrics.result == 'success' && '✅ Success' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Generate Reports | ${{ needs.generate-reports.result == 'success' && '✅ Success' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Send Notifications | ${{ needs.send-notifications.result == 'success' && '✅ Success' || needs.send-notifications.result == 'skipped' && '⏭️ Skipped' || '❌ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## 🔗 Next Steps" >> $GITHUB_STEP_SUMMARY
          echo "- Review artifacts for detailed metrics and dashboard" >> $GITHUB_STEP_SUMMARY
          echo "- Check for any high-priority alerts that need attention" >> $GITHUB_STEP_SUMMARY
          echo "- Review performance recommendations for optimization opportunities" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "For more information, see the [Test Monitoring Guide](./docs/TEST_MONITORING_GUIDE.md)." >> $GITHUB_STEP_SUMMARY

      - name: 🎯 Set workflow conclusion
        run: |
          if [ "${{ needs.collect-metrics.result }}" != "success" ]; then
            echo "❌ Metrics collection failed - this is critical for monitoring"
            exit 1
          elif [ "${{ needs.generate-reports.result }}" != "success" ]; then
            echo "⚠️ Report generation failed - monitoring data collected but reports unavailable"
            exit 0
          else
            echo "✅ Test monitoring completed successfully"
            exit 0
          fi